{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30111d70-cafe-4b88-b970-7b31df2c72d9",
   "metadata": {},
   "source": [
    "# CCT College Dublin\n",
    "\n",
    "## Assessment Cover Page\n",
    "\n",
    "**Module Title**: Machine Learning for AI  \n",
    "**Assessment Title**: ML_CA1  \n",
    "**Lecturer Name**: David McQuaid  \n",
    "**Student Full Name**: Ingrid Menezes Castro  \n",
    "**Student Number**: 2020341  \n",
    "**Assessment Due Date**: 31/05/2024  \n",
    "**Date of Submission**: 31/05/2024  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ad163-5bb2-4391-859e-cfa3ad018639",
   "metadata": {},
   "source": [
    "**GITHUB LINK**: https://github.com/IC2020341/IngridCastro_ML_CA2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36c094-fe0a-446f-9980-40e02a6ad2a9",
   "metadata": {},
   "source": [
    "## Declaration\n",
    "\n",
    "<div style=\"border: 1px solid black; padding: 10px;\">\n",
    "By submitting this assessment, I confirm that I have read the CCT policy on Academic Misconduct and understand the implications of submitting work that is not my own or does not appropriately reference material taken from a third party or other source. I declare it to be my own work and that all material from third parties has been appropriately referenced. I further confirm that this work has not previously been submitted for assessment by myself or someone else in CCT College Dublin or any other higher education institution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a03535b-0367-4c67-b199-acbf36794843",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd64be87-6bbe-4c3d-8705-4a7acd6da170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "\n",
    "\n",
    "# NN\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "# RegressionAlgorithms\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Other\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a4eef-b5e9-4697-b0a8-669abd7d75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb0b31-a7b1-454a-a7a8-18b3815f74f6",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357ef0a-3942-4391-980e-b4d2b2025fc3",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "**1. Neural Networks**\n",
    "- Data Understanding\n",
    "- Data Visualisation\n",
    "- Data Preparation\n",
    "- Neural Networks to predict Income\n",
    "- Regression Algorithm:\n",
    "- Prediction of a New Customer\n",
    "\n",
    "**2. Semantic Analysis**\n",
    "- Data Understanding\n",
    "- Data Preparation\n",
    "- Task 1: Sentiment Analysis\n",
    "- Task 2: Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2402f-6b91-4b57-8c0f-87a0324af3e0",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa330fbd-817e-4099-87de-6a49d75f085d",
   "metadata": {},
   "source": [
    "# 1. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c28eef-d9ab-44e3-ab36-7a47cb670640",
   "metadata": {},
   "source": [
    "## 1.1. Data Understanding\n",
    "\n",
    "In this first part of the data analysis we try to understand what are we dealing with, search for missing/ duplicated/NA values and do some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee5538-eff4-4ade-8665-b253a7932a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"BankRecords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571d7da-b58e-49bb-a69d-15ce0f6f776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da0e60-216c-40a3-9d2c-c3542a926320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827d255-4273-427a-9d43-2f1dd1c007e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00688c-c8d1-4c26-b406-3bbc4afd37d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500db4c-b188-44b2-a6eb-260c17871821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9163d4cf-7c3b-4fea-9d1e-69008baf6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552450a-fc34-43cf-8fb4-c83202d5b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9be03-130f-47ff-b026-2f9c382b2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477fa78a-0787-485d-8620-ac14e51ecd37",
   "metadata": {},
   "source": [
    "## 1.2. Data Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2850b1c-6a18-4aa9-8061-b10c2d1ea5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ad409-5ebd-44a6-881b-b955d6664e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df1.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_columns = df1.select_dtypes(include=['object', 'bool', 'category']).columns\n",
    "\n",
    "df1[numeric_columns].hist(bins=30, figsize=(15, 10), layout=(len(numeric_columns)//3+1, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for column in categorical_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=column, data=df1)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e5be8-7a9f-4102-8dc7-5078141e31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_counts = df1['Age'].value_counts().sort_index(ascending=True)\n",
    "age_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be3bfc-1939-4a5f-beb0-9b6a59245163",
   "metadata": {},
   "source": [
    "Observation: There are negative values for Experience(Years) as should be seen below. This should be treated when Scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8c699-d979-4ba8-9e99-449da57ffae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_counts = df1['Experience(Years)'].value_counts().sort_index(ascending=True)\n",
    "experience_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd315afe-ac68-4229-bb98-b57608e39964",
   "metadata": {},
   "source": [
    "## 1.3. Data preparation\n",
    "\n",
    "In data preparation we need to do the following:\n",
    "- Encode variables;\n",
    "- Scale data;\n",
    "- Prepare for modelling;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4498b-4340-48ff-bf75-43cdf6753c69",
   "metadata": {},
   "source": [
    "For the encoding of variables we need to transform the categorical variables in numerical so we can later scale, split etc. In the next cells we will label encoder the variables:\n",
    "- Personal Loan;\n",
    "- Securities Account;\n",
    "- CD Account;\n",
    "- Online Banking;\n",
    "- and Credit Card;\n",
    "\n",
    "These variables are expressed in 'Yes' or 'No' and we encoded Yes to be 1 and No to be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e3564-a4db-4293-b500-235377e5702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "columns = ['Personal Loan', 'Securities Account', 'CD Account', 'Online Banking', 'CreditCard']\n",
    "\n",
    "for column in columns:\n",
    "    df1[column] = label_encoder.fit_transform(df1[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826019d-8113-4088-8860-3aecccf84da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335ceef-afad-438c-acee-6caf501fb2ed",
   "metadata": {},
   "source": [
    "As you can see above the 'Education' variable is still categorical, so for this one we will apply dummies which will create three new columns:\n",
    "- Education_Degree;\n",
    "- Education_Diploma;\n",
    "- Education_Masters;\n",
    "\n",
    "On the next cell I have transformed the Boolean columns in INT32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5496ba05-9952-416b-9f4a-d9df20aced86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.get_dummies(df1, columns=['Education'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad59bce-7701-47bc-a58b-3059d0c0256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean = ['Education_Degree', 'Education_Diploma', 'Education_Masters']\n",
    "df1[boolean] = df1[boolean].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7695b22b-746a-45db-a63f-86bf100119d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea0b81-1f5d-4f67-b4db-4d7f56ff22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(columns=['Sort Code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337f71a-e2b0-4ebb-b18c-b4ff5d521313",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff87bd0-1b27-4cc7-953b-6d0c15164caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Credit Score'] = (df1['Credit Score'] * 10).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20158017-1b68-4209-99dc-36308b9a9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3f5bb-d871-4acd-8719-277aae8d1942",
   "metadata": {},
   "source": [
    "### Data preparation for modelling and scaling\n",
    "\n",
    "For scaling I will use the MinMaxScaler. The independent Variables (X) are all the other columns but 'Income(Thousands's)', while the dependent variable (y) is 'Income(Thousands's)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8644b8a-4488-4e05-8ae0-7f892261393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.iloc[:, np.r_[0:3, 4:15]]\n",
    "y = df1.iloc[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce245ea-6553-478f-8fd2-04547393d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X= scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3863745e-1c63-4302-b5b4-59cdc175cf54",
   "metadata": {},
   "source": [
    "## 1.4. Neural Networks to predict Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713cbbf4-6f43-40c1-8f83-31fb2e469cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(X.shape[1],), activation='relu'))\n",
    "model.add(LeakyReLU(negative_slope=0.01))\n",
    "model.add(Dense(150))\n",
    "model.add(LeakyReLU(negative_slope=0.01))\n",
    "model.add(Dense(50))\n",
    "model.add(LeakyReLU(negative_slope=0.01))\n",
    "model.add(Dense(50))\n",
    "model.add(LeakyReLU(negative_slope=0.01))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a4793-5a1e-4963-8779-714f82568daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "model.fit(X, y, epochs=500, batch_size=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1a939-b86e-42ff-b2c9-58958dece647",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X,y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c6dbd-4155-427b-adbb-a04ff59a03d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e0048-44ab-425b-b6d9-0cf37ee52928",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226781f-19c3-42a4-b210-1335543fd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.scatter(y, y_pred_nn, color='blue', label='Neural Network Predictions')\n",
    "\n",
    "plt.plot(y, y, color='red', linestyle='--', label='Perfect Predictions')\n",
    "\n",
    "plt.title('Comparison of Predictions: Neural Network')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306fe8a-ad0c-4934-87d4-5c355b97466f",
   "metadata": {},
   "source": [
    "## 1.5. Regression Algorithm to predict Income\n",
    "\n",
    "The first thing we need to do is to see which would be a better fit for this dataset. For that we should compare their efficiency and then optimise and tune our chosen algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d3883-3c4a-4862-9200-bfd1fbf011db",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b2466-82ac-46e9-82f5-2cb244ca9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d45c98-0f32-4e56-9064-b6b79b75bf8d",
   "metadata": {},
   "source": [
    "### Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ddd209-abd5-40d6-98f2-52b93fe91ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "models.append((\"DT\", DecisionTreeRegressor()))\n",
    "models.append((\"RF\", RandomForestRegressor()))\n",
    "models.append((\"LR\", LinearRegression()))\n",
    "models.append((\"RDG\", Ridge()))\n",
    "models.append((\"LSS\", Lasso()))\n",
    "models.append((\"EN\", ElasticNet()))\n",
    "models.append((\"GBR\", GradientBoostingRegressor()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042dd76d-2f8a-46e6-ae4c-e682aa4b4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('%s: Mean MSE = %f, Standard Deviation = %f' % (name, -cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c28a6-1f91-4700-b959-29fbff2fc14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.boxplot(results, labels = names)\n",
    "pyplot.title(\"Algorithm Comparison\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15692eb1-6367-465d-b4a6-c6e388a6fc24",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "\n",
    "The model that performed best was the Random Forest Regressor, so that will be optimized and later compared to the performance of our Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c354a-988b-4ebc-bf02-34a81789a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529fcb6-61f7-446c-bad2-ff5a570404bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_RF_bo = model.predict(X) #model before optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a1d61-f1da-477c-82ec-59540ca56a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_RF_bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b5a8a6-2bd3-46fa-b5b0-83f4c264d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.scatter(y, y_pred_RF_bo, color='blue', label='Random Forest Predictions')\n",
    "\n",
    "plt.plot(y, y, color='red', linestyle='--', label='Perfect Predictions')\n",
    "\n",
    "plt.title('Random Forest Predictions before optimisation')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea53f8-8e13-4f65-a5f7-96d7c82a21f5",
   "metadata": {},
   "source": [
    "### Random Forest Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684c0e2-3b90-45a3-a637-460aeedb1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [2, 4, 6, 8, 10, 12]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 3, 4, 5, 8]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 3]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32d3c6-1d11-49c3-8f27-04162c5617e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "rf_Grid = GridSearchCV(estimator = rf_model, param_grid = param_grid, cv = 3, verbose=2, n_jobs = 4)\n",
    "rf_Grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b232f3-3e72-4da2-a639-5a487f286587",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_Grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f6427-8244-4922-91aa-7b9194d275ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = mean_squared_error(y_train, rf_Grid.predict(X_train))\n",
    "\n",
    "test_mse = mean_squared_error(y_test, rf_Grid.predict(X_test))\n",
    "\n",
    "print(f'Train MSE: {train_mse:.5f}')\n",
    "print(f'Test MSE: {test_mse:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be05011-1a5c-474e-bf00-b33ac518887f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
